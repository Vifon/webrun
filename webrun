#!/usr/bin/env perl

use warnings;
use strict;

use WWW::Curl::Easy;
sub downloader {
    my $curl = WWW::Curl::Easy->new;
    # store a given $url's contents in the $output reference
    sub {
        my ($url, $output) = @_;

        $curl->setopt(CURLOPT_URL       , $url);
        $curl->setopt(CURLOPT_WRITEDATA , $output);
        $curl->perform;
    }
}


use File::Temp qw(tempfile);
use File::Basename;


# saves $data from $url in a temporary file
# and returns its name
sub save_to_temp_file {
    my ($data, $url) = @_;

    # try to guess the downloaded file's extension
    my $suffix = "";
    if (basename($url) =~ /.*(\..*)/) {
        $suffix = $1;
        $suffix =~ s/\?.*//;
    }

    my ($fd, $name) = tempfile(SUFFIX => $suffix,
                               UNLINK => 1);
    print $fd $data;
    close $fd;
    return $name;
}


# We use a new scope to allow the garbage collector to reclaim the unused memory
# when we'll be done with processing ARGV.
#
# I don't know if it works but surely will not hurt.
{
    my $dl = downloader;
    for (@ARGV) {
        # if a given argument is an url
        if (m,^https?://,) {
            my $data;
            # skip failed downloads
            # TODO: should we remove them from ARGV?
            next if $dl->($_, \$data) != 0;
            # replace the url in ARGV with a temporary file
            $_ = save_to_temp_file($data, $_);
        }
    }
}

system @ARGV;
